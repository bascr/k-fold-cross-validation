### K-Fold Cross Validation


In our previous [Walmart Sales](https://github.com/bascr/walmart-sales) analysis project, we focused on removing outliers, evaluating the most meaningful features through RFE (Recursive Feature Elimination) and Feature Engineering, aggregating data and using it to create new historical features to provide more relevant data for the model. We concluded that it was necessary to continue doing other analyses of this dataset changes to determine if our model was memorizing the data (overfitting). In this document, we applied a K-Fold Cross Validation to take advantage of all variability of the data to use it to train and test the model.

[Click here to go to the Jupyter Notebook](https://github.com/bascr/k-fold-cross-validation/blob/main/k-fold-cross-validation.ipynb)